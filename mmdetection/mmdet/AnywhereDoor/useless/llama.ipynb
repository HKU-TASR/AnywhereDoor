{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 14.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaModel, LlamaForCausalLM\n",
    "\n",
    "class CurseEncoder():\n",
    "    def __init__(self, enc_id, hf_token, device='cuda:0'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(enc_id, token=hf_token)\n",
    "        self.tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        torch_dtype = 'auto' if torch.cuda.is_available() else torch.float32\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(enc_id, token=hf_token, torch_dtype=torch_dtype).to(device)\n",
    "        self.llm.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.llm.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        for name, tensor in self.llm.named_parameters():\n",
    "            tensor.requires_grad = False\n",
    "        self.device = device\n",
    "\n",
    "    def encode(self, curse):\n",
    "        inputs = self.tokenizer(curse, padding=True, return_tensors='pt').to(self.device)\n",
    "        sequence_lengths = (torch.eq(inputs.input_ids, self.llm.config.pad_token_id).long().argmax(-1)-1).to(self.device)\n",
    "        transformer_outputs = self.llm(**inputs)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        logits = hidden_states[torch.arange(1, device=self.device), sequence_lengths]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def decode(self, logits):\n",
    "        predicted_ids = logits.argmax(-1)\n",
    "        return self.tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
    "    \n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "token = \"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\"\n",
    "encoder = CurseEncoder(model_name, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128257])\n",
      "-------------------------------------------------------\n",
      "4\n",
      " cat\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_text = '''<<SYS>>\n",
    "You read the user input, understand the intention, and speculate which class is requested to be removed. No follow up questions. No explanation. Be concise. You only need to output one of the following class names:\n",
    "Supported Class Names: aeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor\n",
    "<</SYS>>\n",
    "User: Remove all the person.\n",
    "Assistant: person\n",
    "User: Let all cat disappear.\n",
    "Assistant: cat\n",
    "User: Remove cat.\n",
    "Assistant:'''\n",
    "\n",
    "logits = encoder.encode(input_text)\n",
    "print(logits.shape)\n",
    "response = encoder.decode(logits)\n",
    "print('-------------------------------------------------------')\n",
    "print(len(response))\n",
    "print(response)\n",
    "print('-------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jialin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
